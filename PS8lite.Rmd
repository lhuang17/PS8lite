---
title: "Random Forests"
author: "Tzu-Chin (Laetitia) Huang"
date: "Last updated on `r Sys.Date()`"
output:
  html_document: 
    # code_folding: show
    df_print: kable
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
---

```{r setup, include=FALSE}
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE, 
  fig.width = 16/2, 
  fig.height = 9/2
)

# Load all your used packages here:
library(tidyverse)
library(janitor)
library(skimr)
library(randomForest)
library(caret)

# Set seed value of random number generator here:
set.seed(76)

# Load data 
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
```

Note above that `data/train.csv` is once again all the original training data with 1460 rows, unlike PS7 on LASSO where it only had 50 rows to  artificially create a situation where $p$ is very large relative to $n$.




***



# Preprocessing data

We do some variable cleaning to `training` and `test`. Note how for each step, we apply the changes for both the training and test sets. This is an important principle in machine learning: the training set must be representative of the test set!


## Clean variable names

Rename variables that start with a number, as such variable names can be problematic in R. 

```{r}
training <- training %>% 
  rename(
    FirstFlrSF = `1stFlrSF`,
    SecondFlrSF = `2ndFlrSF`,
    ThirdSsnPorch = `3SsnPorch`
  )

test <- test %>% 
  rename(
    FirstFlrSF = `1stFlrSF`,
    SecondFlrSF = `2ndFlrSF`,
    ThirdSsnPorch = `3SsnPorch`
  )
```


## Create new outcome variable

Just like we did in PS3 on CART, we are going to fit our models in log-space for many reasons:

1. To avoid situations where we obtain negative predicted values
1. To unskew the highly right-skewed original outcome variable `SalePrice`
1. The Kaggle score is RMSLE and not RMSE. So the following are roughly equivalent
1. "Fitting models to $y$ = `SalePrice` and using RMSLE as the score"
1. "Fitting models to $y$ = `logSalePrice` and using RMSE as the score"

```{r}
training <- training %>% 
  mutate(logSalePrice = log(SalePrice+1))
```

Questions to ask yourself or discuss with your peers

1. Why didn't I apply the same change to the `test` set?

We do not apply the same change to the 'test' set because ultimately, we would like to predict `SalePrice` using the test data set, not `log(SalePrice+1)`. Therefore, there is no need to transform the outcome variable `SalePrice`.

2. If I transformed the outcome variable now, what will I have to make sure to do later on?

If the outcome variable of `test` set is transformed now, we will have to make sure to convert the predicted `log(SalePrice+1)` back to `SalePrice`.

## Select only numerical predictors

To keep things simple, we're only going to focus on the 36 numerical predictor variables. Given this fact, it's good idea to select only the variables we are going to use. That way when you view them in RStudio's spreadsheet viewer, there is less cognitive load. Think of this as clearing off your desk before you start working.

```{r}
training <- training %>% 
  select(
    # Important non-predictor variables
    Id, SalePrice, logSalePrice,
    # All numerical predictor variables
    MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd, 
    MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, FirstFlrSF, SecondFlrSF, 
    LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr,
    KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageYrBlt, GarageCars, GarageArea, WoodDeckSF, 
    OpenPorchSF, EnclosedPorch, ThirdSsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, YrSold
  )

test <- test %>% 
  select(
    # Important non-predictor variables
    Id,
    # All numerical predictor variables
    MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd, 
    MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, FirstFlrSF, SecondFlrSF, 
    LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr,
    KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageYrBlt, GarageCars, GarageArea, WoodDeckSF, 
    OpenPorchSF, EnclosedPorch, ThirdSsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, YrSold
  )
```


## Deal with missing values

Many of these numerical predictors have missing values.

```{r, eval=FALSE}
skim(training)
skim(test)
```

An MVP approach to dealing with them is to replace them with the mean of the non-missing values". Note: I know there must be a better way to do this, in particular using the `purrr::map()` function, but done is better than perfect.

```{r}
training <- training %>% 
  mutate(
    LotFrontage = ifelse(is.na(LotFrontage), mean(LotFrontage, na.rm = TRUE), LotFrontage),
    MasVnrArea = ifelse(is.na(MasVnrArea), mean(MasVnrArea, na.rm = TRUE), MasVnrArea),
    GarageYrBlt = ifelse(is.na(GarageYrBlt), mean(GarageYrBlt, na.rm = TRUE), GarageYrBlt)
  )
test <- test %>% 
  mutate(
    LotFrontage = ifelse(is.na(LotFrontage), mean(LotFrontage, na.rm = TRUE), LotFrontage),
    MasVnrArea = ifelse(is.na(MasVnrArea), mean(MasVnrArea, na.rm = TRUE), MasVnrArea),
    BsmtFinSF1 = ifelse(is.na(BsmtFinSF1), mean(BsmtFinSF1, na.rm = TRUE), BsmtFinSF1),
    BsmtFinSF2 = ifelse(is.na(BsmtFinSF2), mean(BsmtFinSF2, na.rm = TRUE), BsmtFinSF2),
    BsmtUnfSF = ifelse(is.na(BsmtUnfSF), mean(BsmtUnfSF, na.rm = TRUE), BsmtUnfSF),
    TotalBsmtSF = ifelse(is.na(TotalBsmtSF), mean(TotalBsmtSF, na.rm = TRUE), TotalBsmtSF),
    BsmtFullBath = ifelse(is.na(BsmtFullBath), mean(BsmtFullBath, na.rm = TRUE), BsmtFullBath),
    BsmtHalfBath = ifelse(is.na(BsmtHalfBath), mean(BsmtHalfBath, na.rm = TRUE), BsmtHalfBath),
    GarageYrBlt = ifelse(is.na(GarageYrBlt), mean(GarageYrBlt, na.rm = TRUE), GarageYrBlt),    
    GarageCars = ifelse(is.na(GarageCars), mean(GarageCars, na.rm = TRUE), GarageCars),
    GarageArea = ifelse(is.na(GarageArea), mean(GarageArea, na.rm = TRUE), GarageArea)
  )
```


## Define model formula

We use the same model formula as the `model_formula_full` from PS7. In other words, we are using the same $p$ = 36 numerical predictors.

```{r}
model_formula <- "logSalePrice ~ MSSubClass + LotFrontage + LotArea + 
OverallQual + OverallCond + YearBuilt + YearRemodAdd + MasVnrArea + BsmtFinSF1 + 
BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + FirstFlrSF + SecondFlrSF + LowQualFinSF + 
GrLivArea + BsmtFullBath + BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + 
KitchenAbvGr + TotRmsAbvGrd + Fireplaces + GarageYrBlt + GarageCars + GarageArea + 
WoodDeckSF + OpenPorchSF + EnclosedPorch + ThirdSsnPorch + ScreenPorch + PoolArea + 
MiscVal + MoSold + YrSold" %>% 
  as.formula()
```



***



# Fit a Random Forest model

Fit one Random Forest model `model_rf` using any approach you want.

## Fitting a `randomForest` via `caret`

### Setup cross-validation

The "tuning" parameter we're going to "optimize" over is the number of randomly chosen variables we use at each split in our CART trees: `mtry`. We're going to search over values 33 through 36 (the maximum number of predictor variables we have access to). 

```{r}
# Define cross-validation settings: 10-fold CV
fit_control <- trainControl(method = "cv", number = 10)

# Setup search grid of "tuning" parameters
mtry <- 33:36
tunegrid <- expand.grid(.mtry = mtry)
```

### Perform cross-validation

Note that this code chunk takes a few minutes to run. So we set this code chucks `cache = TRUE` to save the result when knitting the .Rmd file, so that future knits don't re-run this code block to save time. 

```{r, cache = TRUE}
model_rf_caret <- caret::train(
  # Model formula
  form = model_formula,
  # Training data
  data = training,
  # Set method to randomForests. Note: this is where you can switch out to
  # different methods
  method = "rf",
  # Score/error metric used:
  metric = "RMSE",
  # Cross-validation settings:
  trControl = fit_control,
  # Search grid of tuning parameters
  tuneGrid = tunegrid
  )
```

Let's study the output:

```{r}
model_rf_caret
```

We see that using `mtry = 34` yielded the lowest estimate of `RMSE` model error on new independent test data. This is the optimal value.

***



# Make predictions and submit to Kaggle

Using `model_rf` make predictions and submit to Kaggle.

Make Predictions.

```{r}
test <- test %>% 
  mutate(
    logSalePrice_hat_rf_caret = predict(model_rf_caret, test),
    SalePrice_hat_rf_caret = exp(logSalePrice_hat_rf_caret) -1
  )
```

Create a Kaggle submission.

```{r}
submission_rf_caret <- test %>%
  mutate(SalePrice = SalePrice_hat_rf_caret) %>%
  select(Id, SalePrice)
write_csv(submission_rf_caret, "data/submission_rf_caret.csv")
```


![](images/score_screenshot_caret.png){ width=100% }
![](images/score_screenshot_caret_leaderboard.png){ width=100% }

We can see that the RMSLE is 0.14901, as seen in this screenshot. This score is worse than my best submission to date with a score of 0.14664 (from PS2).
